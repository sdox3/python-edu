{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹스크래핑(Web Scrapping)\n",
    "\n",
    "#### 1. 개념\n",
    "* 웹스크래핑 : 웹사이트상에서 특정 부분에 위차한 정보를 컴퓨터\n",
    "* 웹크롤링\n",
    "\n",
    "\n",
    "#### 3. 파이썬으로 크롤링하기\n",
    "\n",
    "크롤링의 정의는 `크롤링 crawling 또는 스크래핑 scraping`은 웹페이지를 그대로 가져와서 해당 페이지에서 데이터를 추출해 내는 행위이다.\n",
    "\n",
    "#### 크롤링 또는 스크래핑방법\n",
    "1. 원하는 페이지에 request를 보낸 결과를 html로 받는다.\n",
    "2. 받은 html을 파싱한다.\n",
    "3. 필요한 정보를 추출 및 저장\n",
    "\n",
    "파이썬을 이용해서 웹크롤러를 만들기 위해서는 `http request/response`를 다루는 모듈과 html를 파싱하는 모듈이 필요하다.\n",
    "\n",
    "> 참고사이트\n",
    "* https://www.crummy.com/software/Beaurifulsoup\n",
    "* https://docs.python.org/3.0/library/urllib.request/html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. 웹스크래핑  - html소스 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<http.client.HTTPResponse at 0x1c4fb970508>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "b'<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"ko\"><head><meta content=\"text/html; charset=UTF-8\" http-equiv=\"Content-Type\"><meta content=\"/images/branding/googleg/1x/googleg_standard_color_128dp.png\" itemprop=\"image\"><title>Google</title><script nonce=\"tL1jhynnvHAVBR9mIkmfjA==\">(function(){window.google={kEI:\\'beaKXvK0K6GwmAWWlY1g\\',kEXPI:\\'0,202123,3,1151620,5663,731,223,5104,207,3204,10,1051,175,364,1435,4,60,817,383,246,5,767,243,9,335,530,23,107,4,64,41,44,103,5,291,116,5,79,1126354,1197779,245,125,329118,1294,12383,4855,32691,15248,867,28684,363,8825,8384,4859,1361,9290,3030,2843,1895,3118,7910,1,1812,1239,2781,978,7931,5297,2054,622,298,873,1217,1710,1,1264,6430,7432,3874,2884,20,318,4516,2778,520,399,2277,8,2796,885,708,423,856,2213,201,328,149,1103,327,513,124,393,1470,4,48,820,3438,312,1137,2,2063,606,789,1050,184,544,1233,520,1947,747,1462,3,110,328,1284,16,2927,2247,473,1339,1787,3227,773,2072,9,815,6816,6513,2662,642,2449,2459,1226,1462,141,2,3791,1275,108,3407,908,2,940,855,1760,2397,1856,3563,225,996,828,842,480,606,1349,3,346,200,30,156,814,183,562,120,378,1634,1907,439,267,148,189,3312,1462,506,521,32,197,248,910,591,274,121,2285,192,1345,46,996,178,570,4,498,620,219,191,17,126,178,47,275,59,240,353,1009,152,182,483,159,50,190,20,580,289,142,2,132,110,216,69,2,8,41,64,128,309,9,236,383,294,10,113,273,968,4,287,498,240,310,292,1012,88,355,130,28,109,18,94,755,5830637,3276,1802618,6996022,549,333,444,1,2,80,1,900,896,1,8,1,2,2551,1,748,141,59,736,563,1,4265,1,1,1,1,137,1,1193,722,450,229,3,5,7,4,1,15,2,2,1,1,1,2,11,4,1,1,1,5,3,2,3,7,2,1,1,1,1,4,1,10,1,3,23962896,23\\',kBL:\\'-KLY\\'};google.sn=\\'webhp\\';google.kHL=\\'ko\\';})();(function(){google.lc=[];google.li=0;google.getEI=function(a){for(var c;a&&(!a.getAttribute||!(c=a.getAttribute(\"eid\")));)a=a.parentNode;return c||google.kEI};google.getLEI=function(a){for(var c=null;a&&(!a.getAttribute||!(c=a.getAttribute(\"leid\")));)a=a.parentNode;return c};google.ml=function(){return null};google.time=function(){return Date.now()};google.log=function(a,c,b,d,g){if(b=google.logUrl(a,c,b,d,g)){a=new Image;var e=google.lc,f=google.li;e[f]=a;a.onerror=a.onload=a.onabort=function(){delete e[f]};google.vel&&google.vel.lu&&google.vel.lu(b);a.src=b;google.li=f+1}};google.logUrl=function(a,c,b,d,g){var e=\"\",f=google.ls||\"\";b||-1!=c.search(\"&ei=\")||(e=\"&ei=\"+google.getEI(d),-1==c.search(\"&lei=\")&&(d=google.getLEI(d))&&(e+=\"&lei=\"+d));d=\"\";!b&&google.cshid&&-1==c.search(\"&cshid=\")&&\"slh\"!=a&&(d=\"&cshid=\"+google.cshid);b=b||\"/\"+(g||\"gen_204\")+\"?atyp=i&ct=\"+a+\"&cad=\"+c+e+f+\"&zx=\"+google.time()+d;/^http:/i.test(b)&&\"https:\"==window.location.protocol&&(google.ml(Error(\"a\"),!1,{src:b,glmm:1}),b=\"\");return b};}).call(this);(function(){google.y={};google.x=function(a,b){if(a)var c=a.id;else{do c=Math.random();while(google.y[c])}google.y[c]=[a,b];return!1};google.lm=[];google.plm=function(a){google.lm.push.apply(google.lm,a)};google.lq=[];google.load=function(a,b,c){google.lq.push([[a],b,c])};google.loadAll=function(a,b){google.lq.push([a,b])};}).call(this);google.f={};(function(){\\ndocument.documentElement.addEventListener(\"submit\",function(b){var a;if(a=b.target){var c=a.getAttribute(\"data-submitfalse\");a=\"1\"==c||\"q\"==c&&!a.elements.q.value?!0:!1}else a=!1;a&&(b.preventDefault(),b.stopPropagation())},!0);document.documentElement.addEventListener(\"click\",function(b){var a;a:{for(a=b.target;a&&a!=document.documentElement;a=a.parentElement)if(\"A\"==a.tagName){a=\"1\"==a.getAttribute(\"data-nohref\");break a}a=!1}a&&b.preventDefault()},!0);}).call(this);\\nvar a=window.location,b=a.href.indexOf(\"#\");if(0<=b){var c=a.href.substring(b+1);/(^|&)q=/.test(c)&&-1==c.indexOf(\"#\")&&a.replace(\"/search?\"+c.replace(/(^|&)fp=[^&]*/g,\"\")+\"&cad=h\")};</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}\\n</style><style>body,td,a,p,.h{font-family:&#44404;&#47548;,&#46027;&#50880;,arial,sans-serif}.ko{font-size:9pt}body{margin:0;overflow-y:scroll}#gog{padding:3px 8px 0}td{line-height:.8em}.gac_m td{line-height:17px}form{margin-bottom:20px}.h{color:#36c}.q{color:#00c}.ts td{padding:0}.ts{border-collapse:collapse}em{font-weight:bold;font-style:normal}.lst{height:25px;width:496px}.gsfi,.lst{font:18px arial,sans-serif}.gsfs{font:17px arial,sans-serif}.ds{display:inline-box;display:inline-block;margin:3px 0 4px;margin-left:4px}input{font-family:inherit}body{background:#fff;color:#000}a{color:#11c;text-decoration:none}a:hover,a:active{text-decoration:underline}.fl a{color:#36c}a:visited{color:#551a8b}.sblc{padding-top:5px}.sblc a{display:block;margin:2px 0;margin-left:13px;font-size:11px}.lsbb{background:#eee;border:solid 1px;border-color:#ccc #999 #999 #ccc;height:30px}.lsbb{display:block}.ftl,#fll a{display:inline-block;margin:0 12px}.lsb{background:url(/images/nav_logo229.png) 0 -261px repeat-x;border:none;color:#000;cursor:pointer;height:30px;margin:0;outline:0;font:15px arial,sans-serif;vertical-align:top}.lsb:active{background:#ccc}.lst:focus{outline:none}.tiah{width:458px}</style><script nonce=\"tL1jhynnvHAVBR9mIkmfjA==\"></script></head><body bgcolor=\"#fff\"><script nonce=\"tL1jhynnvHAVBR9mIkmfjA==\">(function(){var src=\\'/images/nav_logo229.png\\';var iesg=false;document.body.onload = function(){window.n && window.n();if (document.images){new Image().src=src;}\\nif (!iesg){document.f&&document.f.q.focus();document.gbqf&&document.gbqf.q.focus();}\\n}\\n})();</script><div id=\"mngb\"> <div id=gbar><nobr><b class=gb1>&#44160;&#49353;</b> <a class=gb1 href=\"https://www.google.co.kr/imghp?hl=ko&tab=wi\">&#51060;&#48120;&#51648;</a> <a class=gb1 href=\"https://maps.google.co.kr/maps?hl=ko&tab=wl\">&#51648;&#46020;</a> <a class=gb1 href=\"https://play.google.com/?hl=ko&tab=w8\">Play</a> <a class=gb1 href=\"https://www.youtube.com/?gl=KR&tab=w1\">YouTube</a> <a class=gb1 href=\"https://news.google.co.kr/nwshp?hl=ko&tab=wn\">&#45684;&#49828;</a> <a class=gb1 href=\"https://mail.google.com/mail/?tab=wm\">Gmail</a> <a class=gb1 href=\"https://drive.google.com/?tab=wo\">&#46300;&#46972;&#51060;&#48652;</a> <a class=gb1 style=\"text-decoration:none\" href=\"https://www.google.co.kr/intl/ko/about/products?tab=wh\"><u>&#45908;&#48372;&#44592;</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a href=\"http://www.google.co.kr/history/optout?hl=ko\" class=gb4>&#50937; &#44592;&#47197;</a> | <a  href=\"/preferences?hl=ko\" class=gb4>&#49444;&#51221;</a> | <a target=_top id=gb_70 href=\"https://accounts.google.com/ServiceLogin?hl=ko&passive=true&continue=https://www.google.com/\" class=gb4>&#47196;&#44536;&#51064;</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div> </div><center><br clear=\"all\" id=\"lgpd\"><div id=\"lga\"><img alt=\"Google\" height=\"92\" src=\"/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png\" style=\"padding:28px 0 14px\" width=\"272\" id=\"hplogo\"><br><br></div><form action=\"/search\" name=\"f\"><table cellpadding=\"0\" cellspacing=\"0\"><tr valign=\"top\"><td width=\"25%\">&nbsp;</td><td align=\"center\" nowrap=\"\"><input name=\"ie\" value=\"ISO-8859-1\" type=\"hidden\"><input value=\"ko\" name=\"hl\" type=\"hidden\"><input name=\"source\" type=\"hidden\" value=\"hp\"><input name=\"biw\" type=\"hidden\"><input name=\"bih\" type=\"hidden\"><div class=\"ds\" style=\"height:32px;margin:4px 0\"><div style=\"position:relative;zoom:1\"><input class=\"lst tiah\" style=\"margin:0;padding:5px 8px 0 6px;vertical-align:top;color:#000;padding-right:38px\" autocomplete=\"off\" value=\"\" title=\"Google &#44160;&#49353;\" maxlength=\"2048\" name=\"q\" size=\"57\"><img src=\"/textinputassistant/tia.png\" style=\"position:absolute;cursor:pointer;right:5px;top:4px;z-index:300\" data-script-url=\"/textinputassistant/11/ko_tia.js\" id=\"tsuid1\" alt=\"\" height=\"23\" width=\"27\"><script nonce=\"tL1jhynnvHAVBR9mIkmfjA==\">(function(){var id=\\'tsuid1\\';document.getElementById(id).onclick = function(){var s = document.createElement(\\'script\\');s.src = this.getAttribute(\\'data-script-url\\');(document.getElementById(\\'xjsc\\')||document.body).appendChild(s);};})();</script></div></div><br style=\"line-height:0\"><span class=\"ds\"><span class=\"lsbb\"><input class=\"lsb\" value=\"Google &#44160;&#49353;\" name=\"btnG\" type=\"submit\"></span></span><span class=\"ds\"><span class=\"lsbb\"><input class=\"lsb\" id=\"tsuid2\" value=\"I&#8217;m Feeling Lucky\" name=\"btnI\" type=\"submit\"><script nonce=\"tL1jhynnvHAVBR9mIkmfjA==\">(function(){var id=\\'tsuid2\\';document.getElementById(id).onclick = function(){if (this.form.q.value){this.checked = 1;if (this.form.iflsig)this.form.iflsig.disabled = false;}\\nelse top.location=\\'/doodles/\\';};})();</script><input value=\"AINFCbYAAAAAXor0fY7QF857NQqlYYi2aNoQX-A_6Awe\" name=\"iflsig\" type=\"hidden\"></span></span></td><td class=\"fl sblc\" align=\"left\" nowrap=\"\" width=\"25%\"><a href=\"/advanced_search?hl=ko&amp;authuser=0\">&#44256;&#44553;&#44160;&#49353;</a></td></tr></table><input id=\"gbv\" name=\"gbv\" type=\"hidden\" value=\"1\"><script nonce=\"tL1jhynnvHAVBR9mIkmfjA==\">(function(){var a,b=\"1\";if(document&&document.getElementById)if(\"undefined\"!=typeof XMLHttpRequest)b=\"2\";else if(\"undefined\"!=typeof ActiveXObject){var c,d,e=[\"MSXML2.XMLHTTP.6.0\",\"MSXML2.XMLHTTP.3.0\",\"MSXML2.XMLHTTP\",\"Microsoft.XMLHTTP\"];for(c=0;d=e[c++];)try{new ActiveXObject(d),b=\"2\"}catch(h){}}a=b;if(\"2\"==a&&-1==location.search.indexOf(\"&gbv=2\")){var f=google.gbvu,g=document.getElementById(\"gbv\");g&&(g.value=a);f&&window.setTimeout(function(){location.href=f},0)};}).call(this);</script></form><div id=\"gac_scont\"></div><div style=\"font-size:83%;min-height:3.5em\"><br></div><span id=\"footer\"><div style=\"font-size:10pt\"><div style=\"margin:19px auto;text-align:center\" id=\"fll\"><a href=\"/intl/ko/ads/\">&#44305;&#44256; &#54532;&#47196;&#44536;&#47016;</a><a href=\"http://www.google.co.kr/intl/ko/services/\">&#48708;&#51592;&#45768;&#49828; &#49556;&#47336;&#49496;</a><a href=\"/intl/ko/about.html\">Google &#51221;&#48372;</a><a href=\"https://www.google.com/setprefdomain?prefdom=KR&amp;prev=https://www.google.co.kr/&amp;sig=K_h-DXFhoCaexR8oQ6BMqY1VacZvA%3D\">Google.co.kr</a></div></div><p style=\"font-size:8pt;color:#767676\">&copy; 2020 - <a href=\"/intl/ko/policies/privacy/\">&#44060;&#51064;&#51221;&#48372;&#52376;&#47532;&#48169;&#52840;</a> - <a href=\"/intl/ko/policies/terms/\">&#50557;&#44288;</a></p></span></center><script nonce=\"tL1jhynnvHAVBR9mIkmfjA==\">(function(){window.google.cdo={height:0,width:0};(function(){var a=window.innerWidth,b=window.innerHeight;if(!a||!b){var c=window.document,d=\"CSS1Compat\"==c.compatMode?c.documentElement:c.body;a=d.clientWidth;b=d.clientHeight}a&&b&&(a!=google.cdo.width||b!=google.cdo.height)&&google.log(\"\",\"\",\"/client_204?&atyp=i&biw=\"+a+\"&bih=\"+b+\"&ei=\"+google.kEI);}).call(this);})();(function(){var u=\\'/xjs/_/js/k\\\\x3dxjs.hp.en.Z9GXM1WQNCo.O/m\\\\x3dsb_he,d/am\\\\x3dAAMCbAQ/d\\\\x3d1/rs\\\\x3dACT90oHzsANym_O9pyPQsJs2gvPdWWoEsQ\\';\\nsetTimeout(function(){var b=document;var a=\"SCRIPT\";\"application/xhtml+xml\"===b.contentType&&(a=a.toLowerCase());a=b.createElement(a);a.src=u;google.timers&&google.timers.load&&google.tick&&google.tick(\"load\",\"xjsls\");document.body.appendChild(a)},0);})();(function(){window.google.xjsu=\\'/xjs/_/js/k\\\\x3dxjs.hp.en.Z9GXM1WQNCo.O/m\\\\x3dsb_he,d/am\\\\x3dAAMCbAQ/d\\\\x3d1/rs\\\\x3dACT90oHzsANym_O9pyPQsJs2gvPdWWoEsQ\\';})();function _DumpException(e){throw e;}\\nfunction _F_installCss(c){}\\n(function(){google.spjs=false;google.snet=true;google.em=[];google.emw=false;google.pdt=0;})();(function(){var pmc=\\'{\\\\x22d\\\\x22:{},\\\\x22sb_he\\\\x22:{\\\\x22agen\\\\x22:true,\\\\x22cgen\\\\x22:true,\\\\x22client\\\\x22:\\\\x22heirloom-hp\\\\x22,\\\\x22dh\\\\x22:true,\\\\x22dhqt\\\\x22:true,\\\\x22ds\\\\x22:\\\\x22\\\\x22,\\\\x22ffql\\\\x22:\\\\x22ko\\\\x22,\\\\x22fl\\\\x22:true,\\\\x22host\\\\x22:\\\\x22google.com\\\\x22,\\\\x22isbh\\\\x22:28,\\\\x22jsonp\\\\x22:true,\\\\x22msgs\\\\x22:{\\\\x22cibl\\\\x22:\\\\x22&#44160;&#49353;&#50612; &#51648;&#50864;&#44592;\\\\x22,\\\\x22dym\\\\x22:\\\\x22&#51060;&#44163;&#51012; &#52286;&#51004;&#49512;&#45208;&#50836;?\\\\x22,\\\\x22lcky\\\\x22:\\\\x22I&#8217;m Feeling Lucky\\\\x22,\\\\x22lml\\\\x22:\\\\x22&#51088;&#49464;&#55176; &#50508;&#50500;&#48372;&#44592;\\\\x22,\\\\x22oskt\\\\x22:\\\\x22&#51077;&#47141; &#46020;&#44396;\\\\x22,\\\\x22psrc\\\\x22:\\\\x22&#44160;&#49353;&#50612;&#44032; \\\\\\\\u003Ca href\\\\x3d\\\\\\\\\\\\x22/history\\\\\\\\\\\\x22\\\\\\\\u003E&#50937; &#44592;&#47197;\\\\\\\\u003C/a\\\\\\\\u003E&#50640;&#49436; &#49325;&#51228;&#46104;&#50632;&#49845;&#45768;&#45796;.\\\\x22,\\\\x22psrl\\\\x22:\\\\x22&#49325;&#51228;\\\\x22,\\\\x22sbit\\\\x22:\\\\x22&#51060;&#48120;&#51648;&#47196; &#44160;&#49353;\\\\x22,\\\\x22srch\\\\x22:\\\\x22Google &#44160;&#49353;\\\\x22},\\\\x22ovr\\\\x22:{},\\\\x22pq\\\\x22:\\\\x22\\\\x22,\\\\x22refpd\\\\x22:true,\\\\x22refspre\\\\x22:true,\\\\x22rfs\\\\x22:[],\\\\x22sbpl\\\\x22:16,\\\\x22sbpr\\\\x22:16,\\\\x22scd\\\\x22:10,\\\\x22stok\\\\x22:\\\\x22i8oTVLA9mfZ0IfL6xPpsDorwU0c\\\\x22,\\\\x22uhde\\\\x22:false}}\\';google.pmc=JSON.parse(pmc);})();</script>        </body></html>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "# 사이트에 html 정보를 요청하는 request\n",
    "html = urlopen('https://www.google.com')\n",
    "display(html) # 객체의 정보만 출력\n",
    "html.read() # html를 읽는 함수 == requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "존재하지 않는 사이트 주소입니다.\n"
     ]
    }
   ],
   "source": [
    "# 2. 예외처리\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "try :\n",
    "    # urlopen()에서 리턴되는 객체는 HttpResponse 객체\n",
    "    html = urlopen('https://jaba.com') #에러발생을 위해 만든 없는 사이트 url\n",
    "except HTTPError as e: \n",
    "    print('HTTP 에러!')\n",
    "except URLError as e :\n",
    "    print('존재하지 않는 사이트 주소입니다.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장\n"
     ]
    }
   ],
   "source": [
    "# 3. 이미지 다운로드 방법(1) - 간편한 방법\n",
    "import urllib.request \n",
    "\n",
    "# daum사이트의 로고에서 마우스 우클릭 > 이미지주소 복사\n",
    "url = 'https://t1.daumcdn.net/daumtop_chanel/op/20170315064553027.png'\n",
    "savefilename = './images/daum.png'\n",
    "\n",
    "# url를 일시적으로 저장, file경로를 지정하면  일시저장한 파일을 카피해서 지정경로에 저장\n",
    "\n",
    "urllib.request.urlretrieve(url,savefilename)\n",
    "print('저장')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장\n"
     ]
    }
   ],
   "source": [
    "# 4. 이미지 다운로드 방법(2) - 바이너리파일로 처리\n",
    "\n",
    "# 구글 로고를 저장\n",
    "url = 'https://www.google.com/images/branding/googlelogo/2x/googlelogo_color_272x92dp.png'\n",
    "savefilename = './images/google.png'\n",
    "\n",
    "# 다운로드된 이미지 파일을 메모리에 저장\n",
    "# urllib.request.urlopen(url).read() 앞에는 생략함\n",
    "# byte파일로 저장됨\n",
    "image = urlopen(url).read()\n",
    "# 바이너리파일로 저장\n",
    "with open(savefilename, mode = 'wb') as f :\n",
    "    f.write(image)\n",
    "    print('저장')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stdId=108\n"
     ]
    }
   ],
   "source": [
    "# 5. 매개변수를 추가하여 인터넷 리소스를 요청하는 방법\n",
    "# 기상청의 일기예보 : https://www.weather.go.kr\n",
    "\n",
    "# 기상청 중기 일기예보\n",
    "# 기상청 rss를 검색하면 기상청에서 rss을 제공하는 페이지에 들어갈 수 있다.\n",
    "API = 'http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp'\n",
    "\n",
    "# API에 ?stnId=146를 추가하여 각 지역의 rss를 불러올 수 있다.\n",
    "# url에 특수문자, 한글이 포함된 경웨 URL인코딩이 필요하다.\n",
    "# 지역번호 : 전국 108, 서울/경기 109, 강원 105, 충북 131, 충남 133, 경북 146, 전남, 156\n",
    "# 경북 143, 경남 159, 제주 184\n",
    "\n",
    "values = {'stdId':'108'} # 전국, 각 지역에 맞는 코드를 넣어서 접근\n",
    "\n",
    "# url에 한글, 특수문자가 포함될 경우 encoding\n",
    "# urllib.parse.quote와 같은듯\n",
    "params = urllib.parse.urlencode(values)\n",
    "params\n",
    "# 요청전용 URL 생성\n",
    "url = API + '?' + params\n",
    "print('url =', url)\n",
    "\n",
    "# 다운로드\n",
    "data = urllib.request.urlopen(url).read()\n",
    "\n",
    "# decoding\n",
    "text = data.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 웹스크래핑(scrapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BeautifulSoup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-947bfbeecbf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# requests.get(url).content = urlopen(url).read()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mbs1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# 문서내의 맨 처음의 h1 태그를 선택\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BeautifulSoup' is not defined"
     ]
    }
   ],
   "source": [
    "# 스크래핑 : 웹사이트에서 우너하는 정보를 추출하는 것\n",
    "# BeautifulSope : HTML 파싱 라이브러리\n",
    "# !pip install beautifulsoup4  \n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# https://stackoverflow.com의 메인페이지의 타이틀 텍스트를 가져오기\n",
    "# url에 접속하여 연결된 후에 HttpResponse객체를 생성\n",
    "\n",
    "html = urlopen('https://stackoverflow.com')\n",
    "\n",
    "# html.read() -> html소스\n",
    "# BeautifulSoup의 기본 분석기를 이용하여 html을 분석하기 위한 객체를 생성\n",
    "\n",
    "# 기존 html.read를 보기쉽게 정리\n",
    "# 기존 requests.get = urlopen\n",
    "# requests.get(url).content = urlopen(url).read()\n",
    "\n",
    "bs1 = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "# 문서내의 맨 처음의 h1 태그를 선택\n",
    "print(bs1.h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = Hello Web Scrapping\n",
      "p1 = 웹 페이지 분석\n",
      "p2 = 웹 스크래핑\n"
     ]
    }
   ],
   "source": [
    "# next_sibling\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# 분석하려는 html\n",
    "html = '''\n",
    "<html><body>\n",
    "    <h1 id = 'title'>Hello Web Scrapping</h1>\n",
    "    <p id = 'body'>웹 페이지 분석</p>\n",
    "    <p>웹 스크래핑</p>\n",
    "    \n",
    "</body></html>\n",
    "'''\n",
    "\n",
    "# HTML분석 - html.parser(기본파서) 분석기 사용\n",
    "# Ixml등의 외부 파서도 사용할 수 있다.\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# 원하는 정보를 출력\n",
    "h1 = soup.html.body.h1\n",
    "h1\n",
    "p1 = soup.html.body.p\n",
    "p1\n",
    "\n",
    "# sibling 형제노드(동일 레벨의 모드)\n",
    "# previous_sibiling : 동일 레벨의 이전 노드\n",
    "# next_sibiling : 동일 레벨의 다음 노드\n",
    "# 선택되는 과정 : 첫번째 p 태그의 </p>뒤의 공백문자, 그 뒤의 <p>태그내용\n",
    "\n",
    "# 하나만 할 시 뒤의 공백문자 \\n을 인지해서 \\n이 출력된다.\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "\n",
    "# 요소의 글자를 출력하기\n",
    "# bs(html, 'html.parser').html.body.h1.string\n",
    "# string - 문자열 출력\n",
    "print('h1 =', h1.string)\n",
    "print('p1 =', p1.string)\n",
    "print('p2 =', p2.string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title = 웹 페이지 분석\n",
      "body = 웹 스크래핑\n"
     ]
    }
   ],
   "source": [
    "# 3. find()\n",
    "\n",
    "# html 분석 : html.parser분석기 사용\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# find 매서드로 원하는 부분을 추출\n",
    "# 첫번째 자식객체만 가저옴\n",
    "title = soup.find(id = 'title')\n",
    "body = soup.find(id = 'body')\n",
    "print('title =', p1.string)\n",
    "print('body =', p2.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n",
      "daum = http://daum.net\n",
      "google = http://google.com\n",
      "yahoo = http://yahoo.com\n",
      "naver = http://naver.com\n",
      "nate = http://nate.com\n"
     ]
    }
   ],
   "source": [
    "# 4. find_all()\n",
    "html = '''\n",
    "<html><body>\n",
    "    <ul>\n",
    "        <li><a href ='http://daum.net'>daum</a></li>\n",
    "        <li><a href ='http://google.com'>google</a></li>\n",
    "        <li><a href ='http://yahoo.com'>yahoo</a></li>\n",
    "        <li><a href ='http://naver.com'>naver</a></li>\n",
    "        <li><a href ='http://nate.com'>nate</a></li>        \n",
    "    </ul>  \n",
    "</body></html>\n",
    "'''\n",
    "soup = bs(html, 'html.parser')\n",
    "#soup.find_all?\n",
    "\n",
    "links = soup.find_all('a')\n",
    "# resultset형은 파이썬의 리스트 자료형\n",
    "# 리스트 요소의 타입은 element.Tag 타입\n",
    "\n",
    "print(type(links))\n",
    "for i in links : \n",
    "    a = i.string\n",
    "    # 태그 내부의 속성을 출력 : attrs[속성명]\n",
    "    href = i.attrs['href']\n",
    "    print(a,'=',href)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. css처리 : select_one(), select()\n",
    "\n",
    "html = '''\n",
    "<html><body>\n",
    "    <div id='main'>\n",
    "        <h1>도서 목록</h1>\n",
    "        <ul>\n",
    "            <li>자바 프로그램 입문</li>\n",
    "            <li>파이썬 머신러닝</li>\n",
    "            <li>HTML5.CSS3</li>\n",
    "        </ul>\n",
    "    </div>  \n",
    "</body></html>\n",
    "'''\n",
    "\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# 필요한 부분을 CSS로 추출\n",
    "# h1의 도서 목록 추출, select_one()메서드 활용\n",
    "h1 = soup.h1.string\n",
    "h1 = soup.select_one('div#main>h1').string\n",
    "h1\n",
    "\n",
    "# li 목록 : select\n",
    "li_list = soup.select('div#main>ul>li')\n",
    "print(li_list)\n",
    "for li in li_list :\n",
    "    print(li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습1. 기상청의 일기예보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "○ (강수) 9일(목)은 강원영동에, 12일(일)은 충청도와 전라도, 경북에 비가 오겠습니다.\n",
      "○ (건조) 이번 예보기간(9~16일) 전국이 대체로 맑고 대기가 매우 건조하겠으니, 산불 등 화재예방에 각별히 유의하기 바랍니다.\n",
      "○ (기온) 이번 예보기간(9~16일) 낮 기온은 어제(5일, 9~18도)와 비슷하거나 조금 높은 13~22도로 포근하겠습니다.\n",
      "○ (주말전망) 11일(토)은 대체로 맑겠고, 12일(일)은 충청도와 전라도, 경북에 비가 오겠습니다.\n",
      "              주말 낮 기온은 13~19도로 포근하겠으나, 낮과 밤의 기온차가 10도 이상으로 크겠습니다.\n"
     ]
    }
   ],
   "source": [
    "# 태그의 텍스트 find().string\n",
    "url = 'http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp'\n",
    "html = urlopen(url)\n",
    "soup = bs(html,'html.parser')\n",
    "\n",
    "title = soup.title.string\n",
    "print(title)\n",
    "wf = soup.wf.string\n",
    "wf = wf.replace('<br />', '\\n')\n",
    "#wf = wf.split('<br />')\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습2. 네이버 금융 환율정보\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://finance.naver.com/marketindex/?tabSel=exchange#tab_section'\n",
    "html = urlopen(url)\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "sale = soup.select_one('#exchangeList > li.on > a.head.usd > div > span.value').string\n",
    "print('usd/krw =', sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... 서시\n",
      "... 자화상\n",
      "... 소년\n",
      "... 눈 오는 지도\n",
      "... 돌아와 보는 밤\n",
      "... 병원\n",
      "... 새로운 길\n",
      "... 간판 없는 거리\n",
      "... 태초의 아침\n",
      "... 또 태초의 아침\n",
      "... 새벽이 올 때까지\n",
      "... 무서운 시간\n",
      "... 십자가\n",
      "... 바람이 불어\n",
      "... 슬픈 족속\n",
      "... 눈감고 간다\n",
      "... 또 다른 고향\n",
      "... 길\n",
      "... 별 헤는 밤\n",
      "... 흰 그림자\n",
      "... 사랑스런 추억\n",
      "... 흐르는 거리\n",
      "... 쉽게 씌어진 시\n",
      "... 봄\n",
      "... 참회록\n",
      "... 간(肝)\n",
      "... 위로\n",
      "... 팔복\n",
      "... 못자는밤\n",
      "... 달같이\n",
      "... 고추밭\n",
      "... 아우의 인상화\n",
      "... 사랑의 전당\n",
      "... 이적\n",
      "... 비오는 밤\n",
      "... 산골물\n",
      "... 유언\n",
      "... 창\n",
      "... 바다\n",
      "... 비로봉\n",
      "... 산협의 오후\n",
      "... 명상\n",
      "... 소낙비\n",
      "... 한난계\n",
      "... 풍경\n",
      "... 달밤\n",
      "... 장\n",
      "... 밤\n",
      "... 황혼이 바다가 되어\n",
      "... 아침\n",
      "... 빨래\n",
      "... 꿈은 깨어지고\n",
      "... 산림\n",
      "... 이런날\n",
      "... 산상\n",
      "... 양지쪽\n",
      "... 닭\n",
      "... 가슴 1\n",
      "... 가슴 2\n",
      "... 비둘기\n",
      "... 황혼\n",
      "... 남쪽 하늘\n",
      "... 창공\n",
      "... 거리에서\n",
      "... 삶과 죽음\n",
      "... 초한대\n",
      "... 산울림\n",
      "... 해바라기 얼굴\n",
      "... 귀뚜라미와 나와\n",
      "... 애기의 새벽\n",
      "... 햇빛·바람\n",
      "... 반디불\n",
      "... 둘 다\n",
      "... 거짓부리\n",
      "... 눈\n",
      "... 참새\n",
      "... 버선본\n",
      "... 편지\n",
      "... 봄\n",
      "... 무얼 먹구 사나\n",
      "... 굴뚝\n",
      "... 햇비\n",
      "... 빗자루\n",
      "... 기왓장 내외\n",
      "... 오줌싸개 지도\n",
      "... 병아리\n",
      "... 조개껍질\n",
      "... 겨울\n",
      "... 트루게네프의 언덕\n",
      "... 달을 쏘다\n",
      "... 별똥 떨어진 데\n",
      "... 화원에 꽃이 핀다\n",
      "... 종시\n"
     ]
    }
   ],
   "source": [
    "url = 'https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC'\n",
    "html = urlopen(url)\n",
    "soup = bs(html, 'html.parser')\n",
    "list = []\n",
    "list_1 = soup.find(id = 'mw-content-text')\n",
    "list_2 = list_1.find_all('ul')\n",
    "\n",
    "# ul -> li -> ul -> li\n",
    "# ul -> li -> a\n",
    "# 첫번째가 카테고리화 되어있고 나머지가 아님\n",
    "# 그래서 첫번째꺼에 맞추면 경로가 망해서 selector가 찾지를 못함\n",
    "# find_all에서 ul를 전부 찾는데 ul안에 있는 ul도 카운트함\n",
    "# 그래서 2번째 ul부터 시작하게 만들면 전부 같은 형식의 ul list를 출력이 가능\n",
    "# find_all은 result_set 형식이기 때문에 인덱싱이 가능함\n",
    "# 인덱싱을 통해서 첫번째 다른 형식의 ul를 생략하고 같은 형식의 ul부터 호출하면됨\n",
    "\n",
    "for i in range(1,6):\n",
    "    a = list_2[i].find_all('li')\n",
    "    for j in a :\n",
    "        list.append(j.string)\n",
    "\n",
    "for i in list:\n",
    "    print('...',i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>The Dormouse's story</title>\n",
      "<title>The Dormouse's story</title>\n",
      "title\n",
      "The Dormouse's story\n",
      "The Dormouse's story\n",
      "Elsie\n",
      "Elsie\n",
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister brother\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
      "Elsie\n",
      "Lacie\n",
      "Tillie\n",
      "['sister']\n",
      "['sister']\n",
      "\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister brother\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
      "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister brother\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
     ]
    }
   ],
   "source": [
    "html = '''<!DOCTYPE html>\n",
    "<html>\n",
    "<head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "<p class=\"story\">\n",
    "    Once upon a time there were three little sisters; and their names were\n",
    "    <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "    <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "    <a href=\"http://example.com/tillie\" class=\"sister brother\" id=\"link3\">Tillie</a>;\n",
    "    and they lived at the bottom of a well.\n",
    "</p>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "\n",
    "# 1. element\n",
    "\n",
    "print(soup.title)\n",
    "print(soup.find('title'))\n",
    "\n",
    "# 2. tag\n",
    "\n",
    "print(soup.title.name)\n",
    "\n",
    "# 3. text\n",
    "\n",
    "print(soup.title.string)\n",
    "print(soup.title.get_text())\n",
    "\n",
    "# 4. single element\n",
    "\n",
    "print(soup.a.get_text())\n",
    "print(soup.find('a').get_text())\n",
    "\n",
    "# 5. multi-element\n",
    "\n",
    "print(soup.find_all('a'))\n",
    "\n",
    "for i in range(0,3):\n",
    "    print(soup.find_all('a')[i].get_text())\n",
    "\n",
    "# 6. attribute\n",
    "\n",
    "print(soup.a['class'])\n",
    "print(soup.a.get('class'))\n",
    "print()\n",
    "\n",
    "# 7. find by id\n",
    "\n",
    "print(soup.find(id='link1'))\n",
    "print(soup.find('',{'id':'link1'}))\n",
    "# 8. find by class\n",
    "\n",
    "print(soup.find_all(class_='sister'))\n",
    "print(soup.find_all('',{'class':'sister'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 고급 html 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pavlovna Scherer\n",
      "Empress Marya\n",
      "Fedorovna\n",
      "Prince Vasili Kuragin\n",
      "Anna Pavlovna\n",
      "St. Petersburg\n",
      "the prince\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "the prince\n",
      "the prince\n",
      "Prince Vasili\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "Wintzingerode\n",
      "King of Prussia\n",
      "le Vicomte de Mortemart\n",
      "Montmorencys\n",
      "Rohans\n",
      "Abbe Morio\n",
      "the Emperor\n",
      "the prince\n",
      "Prince Vasili\n",
      "Dowager Empress Marya Fedorovna\n",
      "the baron\n",
      "Anna Pavlovna\n",
      "the Empress\n",
      "the Empress\n",
      "Anna Pavlovna's\n",
      "Her Majesty\n",
      "Baron\n",
      "Funke\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "the Empress\n",
      "The prince\n",
      "Anatole\n",
      "the prince\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "Anna Pavlovna\n"
     ]
    }
   ],
   "source": [
    "# 1. 전쟁과 평화\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "# 전쟁과 평화\n",
    "# http://shop.oreilly.com/product/0636920078067.do 참조 사이트. download ex\n",
    "html = urlopen('https://www.pythonscraping.com/pages/warandpeace.html')\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "\n",
    "# span 태그중에서 class가 green인 태그들\n",
    "# findAll(), find_all()\n",
    "names = soup.find_all('span', class_='green')\n",
    "# or soup.find_all('span', {'class':'green'})\n",
    "\n",
    "for name in names :\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['War and Peace', 'Chapter 1']\n"
     ]
    }
   ],
   "source": [
    "# 2. 웹페이지에서 모든 h태그를 추출\n",
    "# 리스트안에 for문 사용해서 출력\n",
    "titles = soup.findAll(['h1','h2','h3','h4','h5','h6'])\n",
    "print([title.get_text() for title in titles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Well, Prince, so Genoa and Lucca are now just family estates of the\\nBuonapartes. But I warn you, if you don't tell me that this means war,\\nif you still try to defend the infamies and horrors perpetrated by\\nthat Antichrist- I really believe he is Antichrist- I will have\\nnothing more to do with you and you are no longer my friend, no longer\\nmy 'faithful slave,' as you call yourself! But how do you do? I see\\nI have frightened you- sit down and tell me all the news.\", 'Anna\\nPavlovna Scherer', 'Empress Marya\\nFedorovna', 'Prince Vasili Kuragin', 'Anna Pavlovna', 'St. Petersburg', 'If you have nothing better to do, Count [or Prince], and if the\\nprospect of spending an evening with a poor invalid is not too\\nterrible, I shall be very charmed to see you tonight between 7 and 10-\\nAnnette Scherer.', 'Heavens! what a virulent attack!', 'the prince', 'Anna Pavlovna', \"First of all, dear friend, tell me how you are. Set your friend's\\nmind at rest,\", 'Can one be well while suffering morally? Can one be calm in times\\nlike these if one has any feeling?', 'Anna Pavlovna', 'You are\\nstaying the whole evening, I hope?', \"And the fete at the English ambassador's? Today is Wednesday. I\\nmust put in an appearance there,\", 'the prince', 'My daughter is\\ncoming for me to take me there.', \"I thought today's fete had been canceled. I confess all these\\nfestivities and fireworks are becoming wearisome.\", 'If they had known that you wished it, the entertainment would\\nhave been put off,', 'the prince', \"Don't tease! Well, and what has been decided about Novosiltsev's\\ndispatch? You know everything.\", 'What can one say about it?', 'the prince', 'What has been decided? They have decided that\\nBuonaparte has burnt his boats, and I believe that we are ready to\\nburn ours.', 'Prince Vasili', 'Anna Pavlovna', 'Anna Pavlovna', \"Oh, don't speak to me of Austria. Perhaps I don't understand\\nthings, but Austria never has wished, and does not wish, for war.\\nShe is betraying us! Russia alone must save Europe. Our gracious\\nsovereign recognizes his high vocation and will be true to it. That is\\nthe one thing I have faith in! Our good and wonderful sovereign has to\\nperform the noblest role on earth, and he is so virtuous and noble\\nthat God will not forsake him. He will fulfill his vocation and\\ncrush the hydra of revolution, which has become more terrible than\\never in the person of this murderer and villain! We alone must\\navenge the blood of the just one.... Whom, I ask you, can we rely\\non?... England with her commercial spirit will not and cannot\\nunderstand the Emperor Alexander's loftiness of soul. She has\\nrefused to evacuate Malta. She wanted to find, and still seeks, some\\nsecret motive in our actions. What answer did Novosiltsev get? None.\\nThe English have not understood and cannot understand the\\nself-abnegation of our Emperor who wants nothing for himself, but only\\ndesires the good of mankind. And what have they promised? Nothing! And\\nwhat little they have promised they will not perform! Prussia has\\nalways declared that Buonaparte is invincible, and that all Europe\\nis powerless before him.... And I don't believe a word that Hardenburg\\nsays, or Haugwitz either. This famous Prussian neutrality is just a\\ntrap. I have faith only in God and the lofty destiny of our adored\\nmonarch. He will save Europe!\", 'I think,', 'the prince', None, 'Wintzingerode', 'King of Prussia', 'In a moment. A propos,', None, 'le Vicomte de Mortemart', 'Montmorencys', 'Rohans', 'Abbe Morio', 'the Emperor', 'I shall be delighted to meet them,', 'the prince', 'But tell me,', 'is it true that the Dowager Empress wants Baron Funke\\nto be appointed first secretary at Vienna? The baron by all accounts\\nis a poor creature.', 'Prince Vasili', 'Dowager Empress Marya Fedorovna', 'the baron', 'Anna Pavlovna', 'the Empress', 'Baron Funke has been recommended to the Dowager Empress by her\\nsister,', 'the Empress', \"Anna Pavlovna's\", 'Her Majesty', 'Baron\\nFunke', 'The prince', 'Anna\\nPavlovna', 'the Empress', 'Now about your family. Do you know that since your daughter came\\nout everyone has been enraptured by her? They say she is amazingly\\nbeautiful.', 'The prince', 'I often think,', None, 'Anatole', \"Two such charming children. And really you appreciate\\nthem less than anyone, and so you don't deserve to have them.\", \"I can't help it,\", 'the prince', 'Lavater would have said I\\nlack the bump of paternity.', \"Don't joke; I mean to have a serious talk with you. Do you know I\\nam dissatisfied with your younger son? Between ourselves\", \"he was mentioned at Her\\nMajesty's and you were pitied....\", 'The prince', 'What would you have me do?', 'You know I did all\\na father could for their education, and they have both turned out\\nfools. Hippolyte is at least a quiet fool, but Anatole is an active\\none. That is the only difference between them.', 'And why are children born to such men as you? If you were not a\\nfather there would be nothing I could reproach you with,', 'Anna\\nPavlovna', \"I am your faithful slave and to you alone I can confess that my\\nchildren are the bane of my life. It is the cross I have to bear. That\\nis how I explain it to myself. It can't be helped!\", 'Anna Pavlovna']\n"
     ]
    }
   ],
   "source": [
    "# 3. span태그중에서 class가 green, red인 태그\n",
    "# 리스트 for이용해서 출력\n",
    "# find_all('span',{'class':'green','class':'red'}})\n",
    "span = soup.find_all('span',{'class':{'green','red'}})\n",
    "print([spans.string for spans in span])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. wordcount \n",
    "# 'the prince'단어의 갯수를 출력\n",
    "span = soup.find_all('span',text='the prince')\n",
    "len(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<tr><th>\n",
      "Item Title\n",
      "</th><th>\n",
      "Description\n",
      "</th><th>\n",
      "Cost\n",
      "</th><th>\n",
      "Image\n",
      "</th></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift1\"><td>\n",
      "Vegetable Basket\n",
      "</td><td>\n",
      "This vegetable basket is the perfect gift for your health conscious (or overweight) friends!\n",
      "<span class=\"excitingNote\">Now with super-colorful bell peppers!</span>\n",
      "</td><td>\n",
      "$15.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img1.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift2\"><td>\n",
      "Russian Nesting Dolls\n",
      "</td><td>\n",
      "Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \"priceless,\" we mean \"extremely expensive\"! <span class=\"excitingNote\">8 entire dolls per set! Octuple the presents!</span>\n",
      "</td><td>\n",
      "$10,000.52\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img2.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift3\"><td>\n",
      "Fish Painting\n",
      "</td><td>\n",
      "If something seems fishy about this painting, it's because it's a fish! <span class=\"excitingNote\">Also hand-painted by trained monkeys!</span>\n",
      "</td><td>\n",
      "$10,005.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img3.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift4\"><td>\n",
      "Dead Parrot\n",
      "</td><td>\n",
      "This is an ex-parrot! <span class=\"excitingNote\">Or maybe he's only resting?</span>\n",
      "</td><td>\n",
      "$0.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img4.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift5\"><td>\n",
      "Mystery Box\n",
      "</td><td>\n",
      "If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. <span class=\"excitingNote\">Keep your friends guessing!</span>\n",
      "</td><td>\n",
      "$1.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img6.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. 자식노드 수집하기\n",
    "\n",
    "html = req.urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "soup = bs(html,'html.parser')\n",
    "a = soup.find_all(class_='gift')\n",
    "for i in soup.find('table',{'id':'giftList'}).children:\n",
    "     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift1\"><td>\n",
      "Vegetable Basket\n",
      "</td><td>\n",
      "This vegetable basket is the perfect gift for your health conscious (or overweight) friends!\n",
      "<span class=\"excitingNote\">Now with super-colorful bell peppers!</span>\n",
      "</td><td>\n",
      "$15.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img1.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift2\"><td>\n",
      "Russian Nesting Dolls\n",
      "</td><td>\n",
      "Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \"priceless,\" we mean \"extremely expensive\"! <span class=\"excitingNote\">8 entire dolls per set! Octuple the presents!</span>\n",
      "</td><td>\n",
      "$10,000.52\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img2.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift3\"><td>\n",
      "Fish Painting\n",
      "</td><td>\n",
      "If something seems fishy about this painting, it's because it's a fish! <span class=\"excitingNote\">Also hand-painted by trained monkeys!</span>\n",
      "</td><td>\n",
      "$10,005.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img3.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift4\"><td>\n",
      "Dead Parrot\n",
      "</td><td>\n",
      "This is an ex-parrot! <span class=\"excitingNote\">Or maybe he's only resting?</span>\n",
      "</td><td>\n",
      "$0.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img4.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift5\"><td>\n",
      "Mystery Box\n",
      "</td><td>\n",
      "If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. <span class=\"excitingNote\">Keep your friends guessing!</span>\n",
      "</td><td>\n",
      "$1.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img6.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. next_siblings\n",
    "html = req.urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# table태그중에서 id가 giftList인 태그의 자식노드를 추출\n",
    "# 자식노드 bs.find().tr.next_sibilings\n",
    "# sibling : next_sibilings, previouw_siblings\n",
    "# 제목행은 제외하고 검색\n",
    "for sibling in soup.find('table', {'id':'giftList'}).tr.next_siblings:\n",
    "    print(sibling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-449-4d69cf16ec82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msibling\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'src'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'img1.jpg'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msibling\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# 6. previous_sibling\n",
    "html = req.urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "soup = bs(html,'html.parser')\n",
    "\n",
    "\n",
    "# img1.jpg의 부모노드의 이전 형제 노드의 텍스트값\n",
    "# img1.jpg의 prarent, previouw_sibling사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### jason 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"amount\":[{\"num\":0},{\"num\":1},{\"num\":2}],   \"fruits\" : [{\"fruit\":\"apple\"},{\"fruit\":\"banana\"},{\"fruit\":\"pear\"}]}\n",
      "<class 'str'>\n",
      "\n",
      "<class 'dict'>\n",
      "{'amount': [{'num': 0}, {'num': 1}, {'num': 2}], 'fruits': [{'fruit': 'apple'}, {'fruit': 'banana'}, {'fruit': 'pear'}]}\n",
      "{'fruit': 'apple'}\n",
      "{'fruit': 'banana'}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# jason {key:value} 딕셔너리 같은 형태로 들어오는 데이터 형태\n",
    "# 딕 - 리 - 딕 형태로 구성?\n",
    "\n",
    "\n",
    "import json\n",
    "json_str = \\\n",
    "'{\"amount\":[{\"num\":0},{\"num\":1},{\"num\":2}], \\\n",
    "  \"fruits\" : [{\"fruit\":\"apple\"},{\"fruit\":\"banana\"},{\"fruit\":\"pear\"}]\\\n",
    "}'\n",
    "print(json_str)\n",
    "print(type(json_str))\n",
    "print()\n",
    "\n",
    "obj = json.loads(json_str)\n",
    "print(type(obj))\n",
    "\n",
    "print(obj)\n",
    "print(obj.get('fruits')[0])\n",
    "print(obj.get('fruits')[1])\n",
    "print(obj.get('amount')[2].get('num'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pdf분석\n",
    "\n",
    "* pip install pdfminer3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf문서 읽기\n",
    "from pdfminer.pdfinterp import PDFResourceManager, process_pdf\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "from io import open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPDF(pdfFile):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    \n",
    "    process_pdf(rsrcmgr,device,pdfFile)\n",
    "    device.close()\n",
    "    \n",
    "    content = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    \n",
    "    return content\n",
    "\n",
    "# 전쟁과 평화 pdf\n",
    "pdfFile = req.urlopen('http://pythonscraping.com/pages/warandpeace/chapter1.pdf')\n",
    "outputString = readPDF(pdfFile)\n",
    "print(outputString)\n",
    "pdfFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹페이지의 내용을 분석하여 csv파일로 저장\n",
    "# table태그의 내부 텍스트를 저장\n",
    "import csv\n",
    "\n",
    "html = req.urlopen('http://en.wikipedia.org/wiki/Comparison_of_text_editors')\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# class가 wikitable인 태그중에서 첫번쨰 태그를 선택\n",
    "table = soup.findAll('table',{'class':'wikitable'})[0]\n",
    "rows = table.findAll('rt')\n",
    "\n",
    "csv_file = open('../data/editors.csv','wt',newline='', encoding = 'utf-8')\n",
    "writer = csv.writer(csv_file)\n",
    "try:\n",
    "    for row in rows:\n",
    "        csv_row = []\n",
    "        # td, th 태그의 내용을 리스트에 추가\n",
    "        for cell in row.findAll(['td','th']):\n",
    "            csv_row.append(cell.get_text())\n",
    "        writer.writerrow(csv_row)\n",
    "finally : \n",
    "    print('파일이 저장되었습니다.')\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "치매 정도를 'IQ'처럼 숫자로..'조기진단' 가능성 높였다\n",
      "분자도 야구공처럼.. 양자 상태에 따라 운동궤적 달라져\n",
      "배달의민족 \"고개 숙여 사과드립니다\"\n",
      "고개 숙인 배민..\"사장님들 말씀 경청하겠다\"\n",
      "혁신기업들, AI분야 매출액 증가율 73%로 '껑충'\n",
      "최후의 1인을 가린다!카카오 배틀그라운드\n",
      "대체불가 핵앤슬래시!  패스 오브 엑자일\n"
     ]
    }
   ],
   "source": [
    "# 연습문제 1.\n",
    "#다음 사이트에서 링크가 되어 있는 모든 제목을 가져와서 출력하기\n",
    "#http://media.daum.net/digital/\n",
    "\n",
    "html = req.urlopen('https://news.daum.net/digital/')\n",
    "soup = bs(html,'html.parser')\n",
    "\n",
    "a = soup.find_all('div',{'class':'cont_thumb'})\n",
    "\n",
    "for i in a :\n",
    "    b = i.find('a',{'class':'link_txt'}).get_text()\n",
    "    print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 입력코로나\n"
     ]
    }
   ],
   "source": [
    "#연습문제 2.\n",
    "#네이버사이트 이미지 검색후 './images/naver'에 한번에 다운로드 및 저장하기(10ㄱ더\\\n",
    "#https://search.naver.com/search.naver?sm=tab_hty.top&where=image&query=코로나\n",
    "\n",
    "inp = input('검색어 입력')\n",
    "values = {'query':inp}\n",
    "a = urllib.parse.urlencode(values)\n",
    "url = 'https://search.naver.com/search.naver?sm=tab_hty.top&where=image&'\n",
    "url1 = url+a\n",
    "\n",
    "html = req.urlopen(url1)\n",
    "soup = bs(html,'html.parser')\n",
    "a = soup.find_all('img',{'class':'_img'})\n",
    "\n",
    "for i in range(10):\n",
    "    img = a[i]['data-source']\n",
    "    urllib.request.urlretrieve(img,'./images/코로나%d.png'%i)\n",
    "print('저장완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 입력코로나\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://search.naver.com/search.naver?sm=tab_hty.top&where=image&query=%EC%BD%94%EB%A1%9C%EB%82%98'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request \n",
    "\n",
    "# daum사이트의 로고에서 마우스 우클릭 > 이미지주소 복사\n",
    "url = 'https://t1.daumcdn.net/daumtop_chanel/op/20170315064553027.png'\n",
    "savefilename = './images/daum.png'\n",
    "\n",
    "# url를 일시적으로 저장, file경로를 지정하면  일시저장한 파일을 카피해서 지정경로에 저장\n",
    "\n",
    "urllib.request.urlretrieve(url,savefilename)\n",
    "print('저장')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 웹사이트 정보를 mysql로 저장\n",
    "#### 5. 웹사이트 캡춰\n",
    "phantomjs\n",
    "#### 6. 로그인이 필요한 사이트에서 데이타수집\n",
    "크롬에서 네이버로그\n",
    "\n",
    "연습문제\n",
    "1. 도서목록수집(교보문고)\n",
    "2. 다음에서 영화리뷰수집\n",
    "3. 위키백과링크 수집\n",
    "4. 파이썬 크롤링(html, Ajax, JavaScript)\n",
    "5. 웹API로 데이터 추출\n",
    "6. 정기적인 크롤링\n",
    "7. 국제상품가격데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
